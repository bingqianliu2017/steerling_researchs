{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Steerling-8B: Inference Quickstart\n",
    "\n",
    "This notebook demonstrates how to load and use Steerling-8B for:\n",
    "\n",
    "- Text generation\n",
    "- Concept attribution\n",
    "- Embedding extraction\n",
    "- Requirements: GPU with ≥18GB VRAM (A100, A6000, RTX 4090)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install steerling if you haven't already:\n",
    "\n",
    "```bash\n",
    "pip install steerling\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from steerling import SteerlingGenerator, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "First run downloads ~17 GB from huggingface. Subsequent runs load from cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = SteerlingGenerator.from_pretrained(\"guidelabs/steerling-8b\", device=\"cuda\")\n",
    "print(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "Steerling is a causal diffusion model, so it iteratively unmasks output tokens in any order based on model confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic generation\n",
    "text = generator.generate(\n",
    "    \"The key to understanding\",\n",
    "    GenerationConfig(max_new_tokens=50, seed=42),\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation with custom parameters\n",
    "config = GenerationConfig(\n",
    "    max_new_tokens=100,\n",
    "    seed=123,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.2,\n",
    "    use_entropy_sampling=True,  # adaptive temperature based on model uncertainty\n",
    ")\n",
    "text = generator.generate(\"Artificial intelligence will\", config)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Full Generation Output\n",
    "Use `generate_full` to get the full output including token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generator.generate_full(\n",
    "    \"The future of renewable energy\",\n",
    "    GenerationConfig(max_new_tokens=50, seed=42),\n",
    ")\n",
    "print(f\"Text: {output.text}\")\n",
    "print(f\"Prompt tokens: {output.prompt_tokens}\")\n",
    "print(f\"Generated tokens: {output.generated_tokens}\")\n",
    "print(f\"Total tokens: {len(output.tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Generation Parameters Reference\n",
    "\n",
    " | Parameter            | Default | Description                                                        |\n",
    " |----------------------|---------|--------------------------------------------------------------------|\n",
    " | `max_new_tokens`     | 100     | Maximum tokens to generate                                         |\n",
    " | `seed`               | None    | Random seed for reproducibility                                    |\n",
    " | `temperature`        | 1.0     | Sampling temperature (overridden by entropy sampling)              |\n",
    " | `top_p`              | 0.9     | Nucleus sampling threshold                                         |\n",
    " | `top_k`              | None    | Top-k filtering                                                    |\n",
    " | `tokens_per_step`    | 1       | Tokens to unmask per step                                          |\n",
    " | `use_entropy_sampling`| True   | Adaptive temperature (0.3–0.7) based on model uncertainty          |\n",
    " | `repetition_penalty` | 1.2     | Penalty for repeated tokens                                        |\n",
    " | `steer_known`        | None    | Dict of `{concept_id: weight}` for known concept steering          |\n",
    " | `steer_unknown`      | None    | Dict of `{concept_id: weight}` for unknown concept steering        |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
